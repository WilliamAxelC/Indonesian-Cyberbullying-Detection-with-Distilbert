{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "DydZdm9tprFm",
        "outputId": "f85839d7-6c82-4ee6-9f36-4ec94a473ba3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-907070862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset :"
      ],
      "metadata": {
        "id": "e9VlbrWrr9gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/DistilBERT-7-Mei/combined_dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "H2PjfYE7r8vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "smUHCcVg7EHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persiapan Lingkungan"
      ],
      "metadata": {
        "id": "q8JSy6Yp0d5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim tensorflow numpy pandas scikit-learn\n"
      ],
      "metadata": {
        "id": "D_QbmOfssEx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "QBLp-bXV1Yr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "e3oMcDHU1ARD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data Train-Test"
      ],
      "metadata": {
        "id": "JO05hv4_2Rhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = df['clean_text'].astype(str)\n",
        "y = df['encoded_label']\n",
        "\n",
        "# Initialize and fit LabelEncoder to get class names from original labels\n",
        "le = LabelEncoder()\n",
        "le.fit(df['Label'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "LzvWu4f72Sdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenisasi & Padding"
      ],
      "metadata": {
        "id": "j2tJuWeS2bmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 20000   # jumlah kata unik maksimal\n",
        "max_len = 50        # panjang maksimum sequence (padding)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n"
      ],
      "metadata": {
        "id": "nKQqSzTy2dF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download & Load Pre-trained FastText Bahasa Indonesia"
      ],
      "metadata": {
        "id": "g5uhH3td2laY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastText menyediakan pre-trained embeddings untuk >150 bahasa.\n",
        "Untuk Bahasa Indonesia: cc.id.300.vec.gz"
      ],
      "metadata": {
        "id": "FePoS4Hm3Pmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz\n",
        "!gunzip cc.id.300.vec.gz\n"
      ],
      "metadata": {
        "id": "IUQeAe5E2mLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load ke Gensim:"
      ],
      "metadata": {
        "id": "V8tHSynW3Sg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "fasttext_model = KeyedVectors.load_word2vec_format('cc.id.300.vec', binary=False)\n",
        "embedding_dim = 300  # ukuran embedding dari fastText\n"
      ],
      "metadata": {
        "id": "yAWmCdjE3ZX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buat Embedding Matrix"
      ],
      "metadata": {
        "id": "q3qLkszy3r6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "num_words = min(max_words, len(word_index) + 1)\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_words:\n",
        "        continue\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix[i] = fasttext_model[word]\n"
      ],
      "metadata": {
        "id": "bgIOLYoY3stt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bangun Model Bi-GRU"
      ],
      "metadata": {
        "id": "VBfXB93j3ug9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_len,\n",
        "                    trainable=False))  # pre-trained tidak di-train ulang\n",
        "model.add(Bidirectional(GRU(128, return_sequences=False)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # binary classification\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "_Gk4sJey339j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "YlOrayS736Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_pad, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    verbose=1)\n"
      ],
      "metadata": {
        "id": "ndPY0xDQ38R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluasi Model"
      ],
      "metadata": {
        "id": "IXYoT0Z23_KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Get the class names for labels 0 and 1\n",
        "class_names = le.inverse_transform([0, 1])\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "metadata": {
        "id": "ujozmN7d3_2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menyimpan Model dan Objek Pendukung (Tokenizer)"
      ],
      "metadata": {
        "id": "WgREXJmx55hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# 1. Save model Bi-GRU\n",
        "model.save(\"bi_gru_cyberbullying.h5\")  # format HDF5\n",
        "\n",
        "# 2. Save tokenizer atau word_to_index\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)  # jika pakai tokenizer\n"
      ],
      "metadata": {
        "id": "67llQCNc59Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ==== 1. Function Preprocessing Same as Training Data ====\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove punctuation & numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Reusing the preprocess_text function from cell pNmyNKxR6Nnu\n",
        "def preprocess_text(text, tokenizer, max_len):\n",
        "    text = clean_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
        "    return padded\n",
        "\n",
        "\n",
        "# ==== 2. Load Tokenizer & Trained Model ====\n",
        "import pickle\n",
        "with open('tokenizer.pkl', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Load the model with the correct filename\n",
        "model = load_model('bi_gru_cyberbullying.h5')\n",
        "\n",
        "# Ensure maxlen is the same as during training\n",
        "MAXLEN = 50 # Corrected from 100 to 50\n",
        "\n",
        "# Reusing the predict_text function from cell OO3dpisz6U7F\n",
        "def predict_text(model, tokenizer, text, max_len):\n",
        "    processed = preprocess_text(text, tokenizer, max_len)\n",
        "    # Assuming the model outputs probabilities for binary classification\n",
        "    # and the output shape is (batch_size, 1) with sigmoid activation\n",
        "    prob = model.predict(processed)\n",
        "    # The prediction should be based on the probability threshold (e.g., 0.5)\n",
        "    pred = (prob > 0.5).astype(int)[0][0]\n",
        "    return pred, prob.flatten()[0] # Return single probability for binary case\n",
        "\n",
        "\n",
        "# ==== 3. Manual Testing with Full Pipeline ====\n",
        "# Using the predict_text function with the loaded model, tokenizer, and correct max_len\n",
        "test_text1 = \"sudah ka tidak usah digubris tidak usah dipikirin bersyukur saja nikmat tuhan sudah dikasih kaka tidak usah ngatain orang ka tetap bersyukur nikmatnya\"\n",
        "pred1, prob1 = predict_text(model, tokenizer, test_text1, MAXLEN)\n",
        "\n",
        "print(f\"Teks asli: {test_text1}\")\n",
        "print(f\"Teks bersih: {clean_text(test_text1)}\")\n",
        "print(f\"Prediksi: {pred1} (0=non-cyberbully, 1=cyberbully)\")\n",
        "print(f\"Probabilitas: {prob1}\")\n",
        "\n",
        "print(\"-\" * 20)\n",
        "\n",
        "test_text2 = \"kamu bodoh banget\"\n",
        "pred2, prob2 = predict_text(model, tokenizer, test_text2, MAXLEN)\n",
        "\n",
        "print(f\"Teks asli: {test_text2}\")\n",
        "print(f\"Teks bersih: {clean_text(test_text2)}\")\n",
        "print(f\"Prediksi: {pred2} (0=non-cyberbully, 1=cyberbully)\")\n",
        "print(f\"Probabilitas: {prob2}\")"
      ],
      "metadata": {
        "id": "HFn5rbfW-mwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model dan Tokenizer untuk Prediksi Manual"
      ],
      "metadata": {
        "id": "w8KMssGN6JaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ===== Load Model dan Tokenizer =====\n",
        "model = load_model(\"bi_gru_cyberbullying.h5\")\n",
        "\n",
        "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "MAX_LEN = 50  # Harus sama seperti saat training\n",
        "\n",
        "# ===== Preprocessing =====\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # hanya huruf\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # hapus spasi ekstra\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text, tokenizer, max_len):\n",
        "    text = clean_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='pre', truncating='pre')  # samakan dengan training\n",
        "    return padded\n",
        "\n",
        "# ===== Predict Function =====\n",
        "def predict_text(model, tokenizer, text, max_len):\n",
        "    processed = preprocess_text(text, tokenizer, max_len)\n",
        "    prob = model.predict(processed)[0][0]  # output sigmoid, shape=(1,)\n",
        "    pred = 1 if prob >= 0.5 else 0        # threshold 0.5\n",
        "    return pred, prob\n",
        "\n",
        "# ===== Contoh Prediksi =====\n",
        "test_texts = [\n",
        "    \"jelek saja anaknya ayahnya cakep2\",\n",
        "    \"kamu bodoh banget\",\n",
        "    \"muka anak nya ko tua sekali yaa tidak ngegemes..\"\n",
        "]\n",
        "\n",
        "for t in test_texts:\n",
        "    pred, prob = predict_text(model, tokenizer, t, MAX_LEN)\n",
        "    print(f\"Teks asli: {t}\")\n",
        "    print(f\"Teks bersih: {clean_text(t)}\")\n",
        "    print(f\"Prediksi: {pred} (0=non-cyberbully, 1=cyberbully)\")\n",
        "    print(f\"Probabilitas: {prob:.4f}\")\n",
        "    print(\"-\"*30)\n"
      ],
      "metadata": {
        "id": "xVx7v4854QF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred_train = (model.predict(X_train_pad) > 0.5).astype(int)\n",
        "print(confusion_matrix(y_train, y_pred_train))"
      ],
      "metadata": {
        "id": "F5hkJFoC84KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set konfigurasi Git global\n",
        "!git config --global user.email \"immanuel.leonsalomo@gmail.com\"\n",
        "!git config --global user.name \"LeonsMetanoia\"\n",
        "\n",
        "# 2. Clone repo teman\n",
        "!git clone https://github.com/WilliamAxelC/Indonesian-Cyberbullying-Detection-with-Distilbert.git\n",
        "\n",
        "# 3. Copy notebook .ipynb kamu ke dalam folder repo\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Get the path of the current notebook\n",
        "# In Colab, __file__ might not work as expected. A common way is to use the notebook path from the environment.\n",
        "# However, directly getting the current notebook's path in a robust way within a script cell is tricky.\n",
        "# Assuming the notebook is in the default content directory or you know its name:\n",
        "notebook_name = \"Bi-GRU-Updated August.ipynb\" # Replace with your actual notebook name if different\n",
        "notebook_path = f\"/content/{notebook_name}\"\n",
        "\n",
        "# Check if the notebook exists before copying\n",
        "if os.path.exists(notebook_path):\n",
        "    shutil.copy(notebook_path, f\"/content/Indonesian-Cyberbullying-Detection-with-Distilbert/{notebook_name}\")\n",
        "    print(f\"Copied {notebook_name} to the repository folder.\")\n",
        "else:\n",
        "    # If the common path doesn't work, you might need to manually specify or find the path.\n",
        "    # For now, we'll print an error and stop.\n",
        "    print(f\"Error: Notebook file not found at {notebook_path}. Please check the notebook name and path.\")\n",
        "    # Exit or handle the error appropriately if the file is critical.\n",
        "    # For this example, we'll continue to the next steps, but the copy will have failed.\n",
        "\n",
        "\n",
        "# 4. Commit dan push perubahan\n",
        "# Make sure the repository was cloned successfully before changing directory\n",
        "repo_dir = \"/content/Indonesian-Cyberbullying-Detection-with-Distilbert\"\n",
        "if os.path.exists(repo_dir):\n",
        "    %cd {repo_dir}\n",
        "\n",
        "    # Add and commit if the notebook was successfully copied or other changes exist\n",
        "    # Check if there are changes to add before adding and committing\n",
        "    git_status_output = !git status --porcelain\n",
        "    if git_status_output:\n",
        "        !git add .\n",
        "        !git commit -m \"Add notebook from Colab\"\n",
        "        print(\"Changes committed.\")\n",
        "    else:\n",
        "        print(\"No changes to commit.\")\n",
        "\n",
        "\n",
        "    # 5. Push to GitHub with token authentication\n",
        "    from google.colab import userdata\n",
        "    try:\n",
        "        github_token = userdata.get('GITHUB_TOKEN')\n",
        "        if github_token:\n",
        "             # Use the token in the push URL\n",
        "            !git push https://LeonsMetanoia:{github_token}@github.com/WilliamAxelC/Indonesian-Cyberbullying-Detection-with-Distilbert.git main\n",
        "            print(\"Push successful!\")\n",
        "        else:\n",
        "            print(\"Error: GITHUB_TOKEN not found in Colab secrets.\")\n",
        "            print(\"Please add your GitHub Personal Access Token to Colab secrets with the name 'GITHUB_TOKEN'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the push: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Repository directory not found at {repo_dir}. Cloning might have failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzRIBIYxIQT1",
        "outputId": "42871133-48ec-4209-b567-8399d38cc7d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Indonesian-Cyberbullying-Detection-with-Distilbert' already exists and is not an empty directory.\n",
            "Error: Notebook file not found at /content/Bi-GRU-Updated August.ipynb. Please check the notebook name and path.\n",
            "/content/Indonesian-Cyberbullying-Detection-with-Distilbert\n",
            "No changes to commit.\n",
            "Enumerating objects: 3, done.\n",
            "Counting objects: 100% (3/3), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (2/2), done.\n",
            "Writing objects: 100% (2/2), 319 bytes | 319.00 KiB/s, done.\n",
            "Total 2 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/WilliamAxelC/Indonesian-Cyberbullying-Detection-with-Distilbert.git\n",
            "   1eba689..51e7835  main -> main\n",
            "Push successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan notebook aktif ke file .ipynb\n",
        "from google.colab import drive\n",
        "import IPython\n",
        "\n",
        "notebook_name = \"Bi-GRU-Updated August.ipynb\"\n",
        "save_path = f\"/content/{notebook_name}\"\n",
        "\n",
        "# Simpan manual notebook ke file .ipynb\n",
        "IPython.notebook.export_notebook(save_path)\n",
        "print(f\"✅ Notebook disimpan ke {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "I-f-6u1rN-9T",
        "outputId": "c033560f-5252-40e5-a0dc-831019f106fb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'IPython' has no attribute 'notebook'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-214938688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Simpan manual notebook ke file .ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Notebook disimpan ke {save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'IPython' has no attribute 'notebook'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 1. Set konfigurasi Git global\n",
        "# =========================================\n",
        "!git config --global user.email \"immanuel.leonsalomo@gmail.com\"\n",
        "!git config --global user.name \"LeonsMetanoia\"\n",
        "\n",
        "# =========================================\n",
        "# 2. Clone repo teman\n",
        "# =========================================\n",
        "import os\n",
        "\n",
        "repo_dir_name = \"Indonesian-Cyberbullying-Detection-with-Distilbert\"\n",
        "repo_dir_path = f\"/content/{repo_dir_name}\"\n",
        "\n",
        "if not os.path.exists(repo_dir_path):\n",
        "    !git clone https://github.com/WilliamAxelC/Indonesian-Cyberbullying-Detection-with-Distilbert.git\n",
        "    print(f\"✅ Cloned repository to {repo_dir_path}\")\n",
        "else:\n",
        "    print(f\"⚠️ Repository already exists at {repo_dir_path}. Skipping clone.\")\n",
        "\n",
        "# =========================================\n",
        "# 3. Copy notebook ke dalam repo\n",
        "# =========================================\n",
        "import shutil\n",
        "\n",
        "notebook_name = \"Bi-GRU-Updated August.ipynb\"  # nama notebook di Colab\n",
        "src_path = f\"/content/{notebook_name}\"\n",
        "\n",
        "# Biar aman untuk GitHub → ganti spasi dengan \"-\"\n",
        "dst_name = notebook_name.replace(\" \", \"-\")\n",
        "dst_path = f\"{repo_dir_path}/{dst_name}\"\n",
        "\n",
        "if os.path.exists(src_path):\n",
        "    shutil.copy(src_path, dst_path)\n",
        "    print(f\"✅ Copied {notebook_name} → {dst_name} in repo folder.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"❌ Notebook tidak ditemukan di {src_path}. Pastikan nama file benar.\")\n",
        "\n",
        "# =========================================\n",
        "# 4. Commit perubahan\n",
        "# =========================================\n",
        "%cd {repo_dir_path}\n",
        "\n",
        "git_status_output = !git status --porcelain\n",
        "if git_status_output:\n",
        "    !git add .\n",
        "    !git commit -m \"Add {dst_name}\"  # commit message pakai nama file baru\n",
        "    print(\"✅ Changes committed.\")\n",
        "else:\n",
        "    print(\"⚠️ Tidak ada perubahan untuk di-commit.\")\n",
        "\n",
        "# =========================================\n",
        "# 5. Push ke GitHub pakai token dari Colab Secrets\n",
        "# =========================================\n",
        "from google.colab import userdata\n",
        "\n",
        "github_token = userdata.get('GITHUB_TOKEN')\n",
        "if github_token:\n",
        "    push_url = f\"https://LeonsMetanoia:{github_token}@github.com/WilliamAxelC/Indonesian-Cyberbullying-Detection-with-Distilbert.git\"\n",
        "    !git push {push_url} main\n",
        "    print(\"✅ Push berhasil!\")\n",
        "else:\n",
        "    print(\"❌ Error: GITHUB_TOKEN tidak ditemukan di Colab secrets.\")\n",
        "    print(\"Tambahkan token kamu ke Colab (Menu: Runtime → RunTime settings → Secrets).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "fSZY0h_RM-KE",
        "outputId": "15de17c9-d718-41f4-8a94-8a3c62aed992"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Repository already exists at /content/Indonesian-Cyberbullying-Detection-with-Distilbert. Skipping clone.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "❌ Notebook tidak ditemukan di /content/Bi-GRU-Updated August.ipynb. Pastikan nama file benar.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3221004319.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Copied {notebook_name} → {dst_name} in repo folder.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"❌ Notebook tidak ditemukan di {src_path}. Pastikan nama file benar.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# =========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: ❌ Notebook tidak ditemukan di /content/Bi-GRU-Updated August.ipynb. Pastikan nama file benar."
          ]
        }
      ]
    }
  ]
}